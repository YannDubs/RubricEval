gpt4_CoT_v0:
  prompt_template: "gpt4_CoT_v0/prompt.txt"
  fn_completions: "openai_completions"
  completions_kwargs:
    model_name: "gpt-4-turbo-preview"
    max_tokens: 4096
    temperature: 0.3
    function_call:
      name: "evaluate_llm"
    functions:
      - name: "evaluate_llm"
        description: "Evaluate the llm based on one prompt per category."
        parameters:
          type: "object"
          properties:
            clear_goals:
              type: "string"
              description: "The learning objectives of the assignment."
            final_prompt:
              type: "string"
              description: "The prompt that defines the assignment for the model."
            key_criteria:
              type: "array"
              description: "list of all criteria to evaluate the model on"
              items:
                type: "string"
            scoring_scales:
              type: "object"
              description: "Mapping from scales to numeric score. Typically {'Excellent': 4, 'Good': 3, 'Fair': 2, 'Poor': 1}"
            detailed_analytic_rubric:
              type: "object"
              description: "Table of the detailed analytic rubric, in the form of a nested dictionary. The keys of the outer dictionary are the criteria, the keys of the inner one are the scales. The values of the inner dictionary are the detailed descriptions."
        "required": ["clear_goals", "final_prompt", "key_criteria", "scoring_scales", "detailed_analytic_rubric"]
  batch_size: 1
  fn_completion_parser: "json_parser"
  completion_parser_kwargs:
    annotation_key: null
