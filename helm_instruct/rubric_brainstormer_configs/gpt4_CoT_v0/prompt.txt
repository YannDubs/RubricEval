<|im_start|>system
You are a helpful instruction-following assistant.
<|im_end|>
<|im_start|>user
We are performing a holistic evaluation of powerful large language models (LLMs). To do so we need to come up with a challenging prompt/assignment to grade the LLM on, as well as a thorough analytic rubric to help grading the output. The rubric should be written as thoroughly as possible, ideally by experts in the field. Such rubrics will then be used by crowd-worker to grade the LLM's response to the prompt even if they do not know the field well.

Specifically, we will give you an initial prompt that we broadly want to test the model on. You should then output a json that contains the following:
- clear_goals (str): clearly define the learning objectives of the assignment. What should the LLM be able to demonstrate, understand, or produce by the end of the assignment?
- final_prompt (str): the actual prompt that the model will see. This should be related to the initial prompt but can be more specific/detailed to give the LLM information about what we are expecting from it.
- highlevel_criteria (dict[str, str]): Based on the final_prompt (i.e. the assignment), identify the key components or aspects of the assignment that you want to assess and provide clear descriptions of the highleval criteria used to judge each component or aspect.

The json output should then be given to `generate_detailed_rubric(json_output)` to generate detailed rubrics for evaluating the LLM.

Here is the initial prompt:
{prompt}
<|im_end|>