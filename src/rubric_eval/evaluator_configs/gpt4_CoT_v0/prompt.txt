<|im_start|>system
You are a helpful instruction-following assistant.
<|im_end|>
<|im_start|>user
We are performing a holistic evaluation of powerful large language models (LLMs). To do so we need to evaluate the models output based on an assignment and corresponding analytic rubric. We need you to score the LLM's output on each of the criteria of the rubric and give it a score for each of the criteria.

Specifically, you should output a JSON with the following fields:
- `feedback` (dict[str, str]): A dictionary mapping each criteria to a string explaining what the LLM did well and what it did poorly.
- `explanation` (dict[str, str]): A dictionary mapping each criteria to a string explaining why you chose the score you did for that criteria. This should refer to specific parts of the rubric.
- `score_per_criteria` (dict[str, int]): A dictionary mapping each criteria to the score you gave it. The score should be the integer in the scoring scale for that criteria. Typically, this will be a number between 1 and 4.
The JSON should then be given to `final_evaluation_metric(scoring_json)`.

# Assignment
{prompt}

# Rubric
## Key Criteria
{criteria}

## Scoring Scale
{scoring_scales}

## Analytic Rubric
{detailed_analytic_rubric}

# LLM Output
{output}

# Scoring JSON
<|im_end|>