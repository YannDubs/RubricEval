<|im_start|>system
You are a helpful instruction-following assistant and an expert in creating comprehensive, easy-to-apply analytic rubrics for evaluating LLM outputs. Your rubrics will be used by non-expert evaluators, including crowdworkers or other LLMs, who may not have domain-specific knowledge.
<|im_end|>
<|im_start|>user
<instructions>
We are conducting a holistic evaluation of powerful large language models (LLMs). Your task is to create a detailed, user-friendly analytic rubric for evaluating LLM outputs on a given assignment. This rubric will be used by non-expert evaluators (such as crowdworkers) or other LLMs to assess the quality of responses from powerful language models.

Key points about the rubric:
1. Be comprehensive and objective: Cover all aspects of a good response.
2. Be specific to the assignment: Reference concrete elements expected in a correct answer.
3. Design for non-experts: Make it easily applicable by evaluators without domain-specific knowledge.
4. Include clear performance levels: Provide distinct categories for each criterion (e.g., Excellent, Good, Fair, Poor).

When creating the rubric, adhere to these best practices:
- Focus on specific, easily identifiable elements of a good response.
- Provide clear, detailed descriptions of expected performance at each level.
- Use unambiguous language for consistent application by non-experts.
- Cover potential mistakes/properties of a wide range of potential responses.
- Reference specific facts, dates, concepts, or methodologies the LLM should include, providing context if necessary.
- Include guidance on how to identify the quality of each criterion in the LLM's output.
- Provide examples of responses that would fall into each performance level.
- Clearly indicate when a specific element is the only correct answer or when it's an example of a possible correct answer.
</instructions>

<formatting>
# Output Format
Based on the provided assignment, brainstormed rubric and optionally additional useful information about the assignment, generate a JSON output containing the following:
- rubric (list[dict]): A list of dictionaries, each corresponding to a criterion in the rubric. The criteria should be exactly the same as in the expert brainstormed rubric. Each dictionary should have the following keys:
  - criterion (str): The name of the criterion.
  - weight (int): The weight of the criterion in the total score, expressed as an integer between 0-100.
  - performance_to_description (dict[str, str]): A mapping between the performance level ("excellent", "good", "fair", "poor") and a detailed description of what the LLM needs to do to achieve that rating for that criterion. Descriptions must be comprehensive, clear, and refer to specific parts of the ideal response for the criterion. They should be written in a way that non-expert evaluators can easily understand and apply to the LLM's response. "excellent" should give as much information about the ideal response as possible, and others should give as much information about the mistakes compared to "excellent" as possible including mentioning how easy it would be to fix them. Here are all the keys:
    - "excellent" (str): A detailed description of what the LLM needs to do to achieve "excellent" performance for the criterion. This should discuss as much as possible what an ideal response should look like for that criterion. The information and examples should be very concrete to ensure that non-experts can apply it easily.  Give as much concrete information about the the ideal response as possible. Also specify when some errors would not affect 'excellent' for that criteria, e.g., those errors should be penalized in their corresponding criteria.
    - "good" (str): A detailed description of what the LLM needs to do to achieve "good" performance for the criterion. This should discuss what mistakes compared to "excellent" performance would lead to "good" performance. Typically this means having a minor mistake. If so, it should describe what is a minor mistake for that criterion, give a concrete example of what that looks like, and explain how easy it would be to fix such a mistake. Minor mistakes are usually easy to fix with a quick proof-read or slight adjustment.
    - "fair" (str): A detailed description of what the LLM needs to do to achieve "fair" performance for the criterion. This should discuss what mistakes compared to "excellent" performance would lead to "fair" performance. Typically this means having a few minor mistakes, or having a moderate mistake. If so, you should describe what is a moderate mistake for that criterion, give a concrete example of what that looks like, and explain how easy it would be to fix such a mistake. Moderate mistakes require more involved reasoning to fix but are still manageable with a thorough review.
    - "poor" (str): A detailed description of what the LLM needs to do to achieve "poor" performance for the criterion. This should discuss what mistakes compared to "excellent" performance would lead to "poor" performance. Typically this means having a few moderate mistakes, or a major one. If so, you should describe what is a major mistake for that criterion, give a concrete example of what that looks like, and explain how easy it would be to fix such a mistake. Major mistakes significantly impact the correctness and require substantial reasoning to fix.
</formatting>

<example>
# Example
As an example here's a possible machine learning task:

Implement a 3-layer feedforward neural multi-label binary classifier in numpy with the following architecture:```latex
$input\ (\mathbb{R}^{d}) 
\to \mathrm{\mathbf{linear}} 
\to \mathrm{\mathbf{ScaledReLU}} 
\to tmp\ (\mathbb{R}^{h}) 
\to \mathrm{\mathbf{linear}} 
\to \mathrm{\mathbf{ScaledReLU}} 
\to tmp\ (\mathbb{R}^{h}) 
\to \mathrm{\mathbf{linear}}  
\to logits\ (\mathbb{R}^{c})
\to \mathrm{\mathbf{Sigmoid}}
\to prediction \ (\mathbb{R}^{c})
$
```

The loss function to be used for training this network is the mean negative log-likelihood. Below is the final structure of the MLP class. Your job is to fill in the definitions of the functions called by the MLP class.

```python
class MLP:
    def __init__(self, d, h, c):
        self.layers = [
            Linear(d, h), ScaledReLU(h),  
            Linear(h, h), ScaledReLU(h),  
            Linear(h, c), Sigmoid(c)  
        ]
        
    def forward(self, X):
        """Predict classes for a batch of n examples"""
        out = X  # $\in \mathbb{R}^{n \times d}$
        for layer in self.layers:
            out = layer.forward(out) 
        return out
    
    def sgd(self, X_train, Y_train):
        ...

    def single_step_sgd(self, X, Y):  # $X\in \mathbb{R}^{n\times d}$, $Y\in \mathbb{R}^{n\times c}$
        """Train the MLP with a single step of batched SGD."""
        Y_hat = self.forward(X)  # $\in \mathbb{R}^{n \times c}$
        loss_nabla_O = get_grad_log_loss(Y, Y_hat)
        for layer in self.layers[::-1]:  # backward pass reverses layers
            loss_nabla_O, *param_grads = layer.backward(loss_nabla_O)
            layer.update_params_(*param_grads, lr=0.1)
```

You should write the `forward`, `backward`, `update_params_`, and `__init__` methods for the `Linear`, `ScaledReLU`, and `Sigmoid` classes, as well as the `get_grad_log_loss` function. Note that `ScaledReLU` is similar to ReLU, but it scales the output by 2 if the input is greater than 1 and 0 otherwise. For the linear layer, use Kaiming initialization for the weights and zero initialization for the bias. Use activation checkpointing to save memory wherever possible. Implement everything using NumPy. Your code should be fully functional.

Considering only matrix multiplications in your code, approximately how much slower (in terms of FLOPs) would it be to train our MLP on $n = 10,000$ examples versus performing inference on $n = 10,000$ examples? Your answer should be an integer (round it if you get a float). Provide your derivation.

# Possible rubric
Here's a possible analyticrubric for this assignment:
[
  {
    "criterion": "Following Assignment Instructions",
    "weight": 10.0,
    "performance_to_description": {
      "excellent": "The code attempts to implement all required functions and classes: Linear, ScaledReLU, Sigmoid, and get_grad_log_loss. It uses activation checkpointing, Kaiming initialization for weights, and zero initialization for biases. The FLOPs calculation considers only matrix multiplications, and the final answer is an integer. The code is fully functional and follows all instructions in the prompt.",
      "good": "The code implements most of the required functions and classes, with 1 minor omission. For example, it might have forgotten to zero initialize the bias or it returned a decimal number ratio of training to inference instead of an integer. Errors for this criterion are not about implementation but about missing details in the instructions. Minor omissions are those that would lead to very similar solutions and would have trivial fixes.",
      "fair": "The code implements some of the required functions and classes, but has one moderate omission or two minor ones. For example, it didn't attempt Kaiming initialization (moderate omission) or implemented ReLU instead of ScaledReLU (moderate omission). Multiple small omissions may include not initializing the bias (minor omission) and considering non-matrix multiplications in FLOP calculation (minor omission). Moderate omissions are those that would lead to different solutions and would require some thinking to fix.",
      "poor": "The code has a major omission or multiple moderate ones. For example, it didn't implement activation checkpointing at all (major omission), the code uses pseudocode (major omission), or one of the required functions or classes is missing (major omission). Major omissions are those that would lead to significantly different solutions and that would require significant thinking to fix."
    }
  },
  {
    "criterion": "Implementation of the Forward Pass",
    "weight": 20.0,
    "performance_to_description": {
      "excellent": "The forward pass for each layer is correctly implemented: Linear as `X @ W + b`, ScaledReLU as `X * 2 if X > 1 else 0`, and Sigmoid as `1 / (1 + np.exp(-X))`. Activation checkpointing is correctly used in the forward pass: Linear saves self.X, ScaledReLU saves the mask, and Sigmoid saves the output self.O. Biases are initialized to zero using `np.zeros(h)` and weights are initialized using Kaiming initialization `np.random.randn(input_dim, output_dim) * np.sqrt(2. / input_dim)`.",
      "good": "The forward pass is mostly correct, with 1 minor error. For example, the mask is not saved in ScaledReLU (activation checkpointing). Minor errors are those that would nearly not affect the final code and solution (e.g., activation checkpointing for ScaledReLU) or use `np.sqrt(1. / input_dim)` instead of `np.sqrt(2. / input_dim)` in Kaiming initialization.",
      "fair": "The forward pass is partially correct, with one moderate error or two minor ones. For example, scaled ReLU doesn't do what it's supposed to (moderate error), or Kaiming initialization is completely wrong (moderate error), or the sigmoid forgot to take the negative sign in the exponent (moderate error), or using shape mistakes e.g. `W@X + b` instead of `X @ W + b` in the linear layer. Moderate errors are either those that could give different final answers (e.g., forgetting to initialize the weight, or forgetting the sign in the exponent) but can be easily fixed, or those that would fail loudly and are close to being correct e.g., a shape mistake.",
      "poor": "The forward pass contains multiple moderate errors or a major one. Major errors are those that would require significant thinking to fix, e.g., `O = 1 / (1 + np.exp(-X))` is not implemented or is completely wrong."
    }
  },
  {
    "criterion": "Implementation of the Backward Pass and Gradient Calculation",
    "weight": 35.0,
    "performance_to_description": {
      "excellent": "The backward pass for each layer is correctly implemented: Linear layer gradients are calculated correctly using the chain rule (e.g., of correct implementation `dL_dW = self.X.T @ dL_dY`, `dL_db = dL_dY.sum(axis=0)`, `dL_dX = dL_dY @ W.T` where `self.X` is checkpointed/stored in the forward pass), ScaledReLU backward pass correctly applies the double gradient only where the input was greater than 1 (e.g., `dL_dX = dL_dY * self.scaled_mask` where `self.scaled_mask = (X > 1) * 2` is checkpointed/stored in forward pass), and Sigmoid backward pass correctly applies the derivative of the sigmoid function (e.g., `dL_dX = dL_dY * self.O * (1 - self.O)` where `self.O=1 / (1 + np.exp(-self.X))` is checkpointed/stored in forward pass). The get_grad_log_loss function accurately computes the gradient of the log loss with respect to the weights (e.g., `1/len(Y) * (Y_hat - Y) / ((Y_hat * (1 - Y_hat)) + eps)` including the normalization by batch size). Stochastic Gradient Descent (SGD) is correctly applied by updating the weights using the computed gradients and a learning rate (e.g., `self.W -= lr * dL_dW` and `self.b -= lr * dL_db`). Activation checkpointing is used in the backward pass to save intermediate activations and those are reused in the backward pass.",
      "good": "The backward pass is mostly correct, with minor errors in one backward pass. For example, one of the intermediate activations is not reused in the backward pass, or `get_grad_log_loss` forgot to normalize by the batch size. Minor errors are those that would nearly not affect the final code / solution and are easy to fix, or those that would fail loudly and thus be easy to spot (e.g., matrix multiplication wouldn't run due to shape mismatch).",
      "fair": "The backward pass has one moderate error or two minor ones. Moderate errors are those that could give very different final answers (e.g., the sign of the update in SGD is wrong, or the chain rule is not applied correctly `dL_dX = self.O * (1 - self.O)`) but are easy to fix (the solution is close to correct), or errors that would fail loudly and thus be easy to spot (e.g., matrix multiplication wouldn't run due to shape mismatch).",
      "poor": "The backward pass contains multiple moderate errors or a major one. Major errors are those that would require significant thinking to fix, e.g., `loss_nabla_X = loss_nabla_O @ self.W.T` is not implemented or is completely wrong."
    }
  },
  {
    "criterion": "Overall Code Quality",
    "weight": 5.0,
    "performance_to_description": {
      "excellent": "The code is fully functional, efficient, and follows best practices in Python programming. It uses vectorized operations instead of loops, handles potential numerical instabilities (e.g., using epsilon in get_grad_log_loss), and uses clear, descriptive variable names. The code is simple, concise, and easy to understand, with no unnecessary or irrelevant details. For example, it adds useful/concise documentation when needed and arguments to functions when needed.",
      "good": "The code is functional and mostly follows best practices, with one minor issue. For example, it may forget the epsilon in get_grad_log_loss, use unnecessarily many lines of code, or doesn't type the arguments to functions. The code is generally easy to understand but may have a few unnecessary details. Minor issues are those that would most often not affect the final outcome of the code and are easy to fix.",
      "fair": "The code is mostly functional but has two minor issues or one moderate one. For instance, it might use loops instead of vectorization somewhere (moderate issue), or have unclear variable names throughout (minor issue) as well as lacking docstrings (minor issue). The code may be somewhat difficult to understand or contain several unnecessary details. Moderate issues are those that would significantly impact the final outcome of the code but are not wrong (e.g., slow due to a for loop), or those that significantly impact the code quality but are easy to fix.",
      "poor": "The code contains multiple moderate errors or a major one. Major errors are those that would require significant refactoring to fix and significantly impact the code quality."
    }
  },
  {
    "criterion": "Computational Complexity Analysis",
    "weight": 25.0,
    "performance_to_description": {
      "excellent": "The analysis correctly considers only the linear layer for matrix multiplications. It accurately calculates the FLOPs for forward pass as 2nio, backward pass as 4nio (2nio for gradients w.r.t. activations and 2nio for gradients w.r.t. weights), and total training as 6nio (forward + backward). It correctly determines the ratio of training to inference FLOPs as 3 (6nio / 2nio), and notes that this ratio is independent of n for large n (or equivalent to only considering matrix multiplications). The explanation is clear, concise, and shows a deep understanding of the computational complexity involved, including the reasoning behind the 2nio approximation for matrix multiplication (no dot products, each for vectors of shape i, and dot product of two vectors of dimensionality d is approx 2d).",
      "good": "The analysis is mostly correct, with one minor issue. For example, it may have the correct equations but have some computation error (minor issue), or may not recognize that the ratio is independent of n for large n (or rather large n is equivalent to only considering matrix multiplications) (minor issue). Minor issues are such that the overall reasoning is sound and a simple proofreading would help find the error.",
      "fair": "The analysis contains a moderate error. For instance, a common moderate error is to consider training to only perform the backward pass (rather than forward + backward) which would lead to conclude that the ratio is $4nio/2nio=2$ rather than $3$. Another common moderate error would be to consider the forward pass as $nio$ rather than $2nio$ (common and easy to fix error). Another common error may be to only consider the gradients w.r.t. activations in the backward pass (rather than both activations and weights) which would be a 2nio rather than 4nio. Moderate errors uncover a lack of understanding of at least one thing but are easy to fix.",
      "poor": "The analysis contains multiple moderate errors or a major one. Major errors are those that require significant thinking to fix. For example, the answer gets the number of FLOPS in the backward pass completely wrong."
    }
  },
  {
    "criterion": "Clarity and Conciseness of Explanation",
    "weight": 5.0,
    "performance_to_description": {
      "excellent": "The explanation is clear, concise, and easy to follow. It avoids irrelevant or unnecessary details, focuses on the key concepts and calculations, and presents information in a logical order. The language used is precise and appropriate for the technical nature of the topic.",
      "good": "The explanation is mostly clear and concise, with a minor issue. A minor issue may be that it includes a few unnecessary details or has slight inconsistencies in the logical flow, but overall it's easy to follow and focuses on the key points.",
      "fair": "The explanation is somewhat clear but has one moderate issue or two minor ones. For example, it might include several irrelevant details, skip over some important points, or present information in a confusing order. The language might be imprecise or inconsistent in places. Moderate issues are those that would still make the answer understandable but significantly impact the clarity and quality of the explanation. These should be relatively easy to fix.",
      "poor": "The explanation contains major issues that make it unclear, verbose, or difficult to follow. It might include many irrelevant details, miss crucial points, or present information in a disorganized manner. The language used might be inappropriate or consistently imprecise, making it hard to understand the key concepts and calculations. Major issues are those that would require significant thinking to fix and significantly impact the clarity and quality of the explanation."
    }
  }
]
</example>

<task>
# Actual assignment for which to generate rubrics
{instruction}
</task>

<information>
# Additional information about the assignment
## Brainstormed Rubric
{brainstormed_rubric}

## Useful information to evaluate the prompt's responses (optional)
{useful_info_to_eval_instruction}
</information>


Now, based on this assignment and brainstormed rubric, create a comprehensive, easy-to-apply analytic rubric using the format described. Ensure that your rubric covers all aspects of the assignment, provides clear guidance for non-expert evaluators, and offers a fair and thorough evaluation framework. Once complete, call the `evaluate_llm(json_output)` function to evaluate the LLM's responses. 

Remember that the evaluation will be done by non-expert so the rubric should contain as much detail as possible about the ideal response and be concrete about the type of errors. Given that non-experts may not even know the correct answer, the "performance_to_description" should give as much information (the actual answers / facts) about the ideal response as possible under "excellent" performance. All the other levels of performance should describe the mistakes that need to be made to get that rating. Don't worry about adding too much detail in the description of the rubric! In particular, all information in "brainstormed_rubric" should absolutely be included in the "performance_to_description"!
<|im_end|>