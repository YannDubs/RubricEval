gpt-4o-2024-08-06_CoT_v0:
  prompt_template: "gpt-4o-2024-08-06_CoT_v0/prompt.txt"
  fn_completions: "openai_completions"
  completions_kwargs:
    model_name: "gpt-4o-2024-08-06"
    max_tokens: 8192
    temperature: 0.0
    tool_choice:
      type: function
      function:
        name: "make_evaluation_report"
    tools:
      - type: function
        function:
          name: "make_evaluation_report"
          description: "Evaluate the LLM's output on the current assignment given a list-view rubric with deductive scoring."
          strict: true
          parameters:
            type: "object"
            properties:
              explanation:
                type: "string"
                description: "A concise explanation of the LLM's performance, referencing specific parts of the LLM's output and the rubric description. This should mention all the error_name entries from the rubric that are being deducted, as well as any additional errors or outstanding performances not covered by the rubric if applicable."
              list_error_rubric_grading:
                type: "array"
                description: "A list of dictionaries, each corresponding to an error, omission, or exceptional performance that actually applies to the output. Include only relevant items from the provided List-view Rubric with Deductive Scoring or additional observations not covered by the rubric. Do not include errors that do not apply to the output."
                items:
                  type: "object"
                  description: "An error, omission, or exceptional performance that applies to the output. If it doesn't apply (delta_score zero) then don't include it!"
                  properties:
                    error_name:
                      type: "string"
                      description: "The error_name from the rubric or a new error_name for additional observations."
                    is_present_in_rubric:
                      type: "boolean"
                      description: "Whether the item is present in the original list-view rubric (true) or is an additional observation (false)."
                    concise_explanation:
                      type: "string"
                      description: "Concise explanation about why the `error_name` applies, highlighting the specific part of the LLM's output that is incorrect or problematic."
                    optional_comment:
                      type: "string"
                      description: "Rare additional comment about the item. Use an empty string ('') unless there's an exceptional case that requires explanation."
                    delta_score:
                      type: "number"
                      description: "The non-zero score to be applied. Negative for deductions, positive for exceptional performances that warrant additional points."
                  additionalProperties: false
                  required: ["error_name", "is_present_in_rubric", "concise_explanation", "optional_comment", "delta_score"]
            additionalProperties: false
            required: ["explanation", "list_error_rubric_grading"]
  batch_size: 1
  fn_completion_parser: "json_parser"
  completion_parser_kwargs:
    annotation_key: "list_error_rubric_grading"
